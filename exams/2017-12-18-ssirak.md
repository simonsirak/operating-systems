(kolla upp rpm o sånt, samt om lru är segt pga minnesanrop OCKSÅ eller huvudsakligen pga bara pga tidskomplexiteten, samt också de där bananreglerna för MLFQ som inte var bra)

# 1
## 1.1
bar är en mapp. foo är en fil. kalle äger bar, jonny äger foo. alla i gruppen admin kan läsa, skriva, komma åt bar, resten kan bara lista och komma åt saker i bar, inte skriva (aka lägga till nya filer vilket skriver till directoriet). För filen foo kan jonny och alla i angels läsa och skriva, och resten kan bara läsa.

## 1.2
ln skapar hård link till en fil. chown ändrar ägaren till en fil. less är ett tool som visar skrollbar text i terminalen. pwd printar ut present working directory.

# 2
## 2.1
Vi returnerar en array som allokerats på stacken. När funktionen är färdig kommer stack-ramen "kastas bort" (i.e stack pointer kommer sjunka ned till under stack frame), och kan köras över i framtiden. Så buffer kan innehålla garbage redan efter nästa funktionsanrop.

## 2.2 *
Att den är priviligerad innebär att man inte kan köra den i user mode, utan måste höja priviligienivån till kernel mode för att genomföra den. Den behöver vara priviligerad eftersom annars skulle vilken process som helst kunna ersätta IDT:n med en IDT som pekar till egna handlers, och ta över processorn totalt.

# 3
## 3.1
Vi skulle kunna implementera en "simpel" MLFQ. Denna utgår ifrån att en process är interaktiv, och sedan uppfattar den mer med tiden. Heuristiken som används är att lägga alla nya processer i högsta prioriteten. Om en hel tidslice har körts utan IO sänker man prio med ett snäpp. Man skulle sedan kunna höja upp alla processer till högsta prioritet efter ett tag ifall processser har blivit mer interaktiva över tiden.

## 3.2
Kan.

## 3.3 *
Man måste beräkna det nya antal enheter man har gått genom att plussa på sitt steg till sina tidigare. Man lägger sedan processen i en position sådan att processen efter har gått längre, och processen före har gått kortare.

# 4
## 4.1
Paging delar upp minnet i lika stora delar, så det kan aldrig bli att en minnesyta är för liten för att utnyttjas; varje gång en enhet minne efterfrågas så är det i form av sidor. Extern fragmentering är just att för små minesytor (mellan allokerade minnesytor) inte kan utnyttjas, men det kan inte hända med paging.

## 4.2
En trädstruktur gör att vi kan lagra page table mycket mer effektivt i minnet. Ett directory i trädstrukturen håller då ett antal sidor, lika stora som vanliga sidor. Varje sida i directory kan då (om 1 fysisk sida = 4KiB och en entry är 4 bytes) lagra pekare till 1024 fysiska sidor. Så en sida på 4KiB behöver bara lagras i minnet för att kunna komma åt 4MiB av information, istället för att lagra allt i minnet. En TLB hjälper ändå till att minska antalet indirektioner som sker i trädstrukturen, så det är inte SÅ illa. Minnesminskningen av bara ett indirektionssteg är mycket viktigare.

## 4.3 *
Vi kan lägga till mer minne, så att vi kan få plats med mer processer utan att löpa stor risk för alltför mycket page replacement. Detta eftersom PTE:s tillåter (om TLB också tillåter) oss att addressera ett större fysiskt minne, främst ett större antal sidor (12 bitar för sidstorleken är ju ganska normalt). 

# 5
## 5.1
Systemanrop är sega. malloc tar hänsyn till minne som redan allokerats (men sedan befriats) när den ger minne till användaren. malloc tar även ut mer minne än vad som behövs vid anropet. Detta medför att vi använder minnet mer hänsynsfullt, men även att operationen går snabbare eftersom vi inte behöver göra systemanrop hela tiden.

## 5.2
0b001000 är vår buddy (man flippar biten motsvarande djupet från basenheten. 16 byte-block är första nollan, andra biten är för 32 bytes block).

## 5.3 *
För att hålla den sorterad så kan man istället för att lägga det fria blocket i början av listan lägga in den sådan att förra elementet har en lägre address och nästa har en högre address (likhet finns inte om vi inte har en bugg). Allokering görs som med en osorterad lista. När vi friat blocket och satt in det så kan vi direkt kolla grannarna och se om de är direkt angränsade (man utnyttjar deras storleksattribut och storleken av ett element i freelistan). Om de är det så slår man ihop blocken genom att lägga till minnet från den "högra" blocket (inklusive frielementstrukturens bytes) till det "vänstra" blocket. Denna check behöver bara göras på de två direkta grannarna, eftersom man vet att alla andra element garanterat inte kan mergeas (eftersom de inte redan är mergeade från förut). Detta är snabbt.

# 6
## 6.1
Resultatet blir "the count is: 10" på två rader. Detta eftersom count är en stack-variabel, och två trådar delar inte stack (men kan komma åt varandras stack eftersom de delar processens minne -- men det görs inte här).

## 6.2
Ja, så länge man har en pekare till strukturen (t.ex genom en global pekare).

## 6.3 *
Vet inte faktiskt

# 7
# 7.1
Rotationshastigheten är en stor del av kostnaden för att komma åt en sektor på en hårddisk. En snabbare rotation gör att läsning/skrivning av en sektor kan bli snabbare. Den förbättrar alltså seek time. En väldigt snabb rotationshastighet kan dock försämra läs/skrivhastigheten, om man inte hinner med rotationen av en skiva.

## 7.2
Vi kommer att ändra i inode, data bitmap och datablock. data bitmap måste allokera ett datablock, inode måste peka till datablocket (och även uppdatera modifieringstid men det görs antagligen i början/slutet). Datablocket måste sedan skrivas till (av det vi ville skriva).

## 7.3 * 
Har skippat detta

# 8
## 8.1
hypervisor hittar int. handler för det anropet för det OS:et, och låter OS:et exekvera den i user mode. Sedan när OS:en är klar kommer hypervisor fånga return from trap och göra den riktiga return from trap och låta appen resume execution.

## 8.2 *
Problemet: priviligerad operation, man vill inte att ett OS ska kunna ändra sånt då den kan ta över systemet då. Man låter då hypervisor ta detta priviligerade anrop, lagra IDT:n i en struktur som representerar VOS, och sen använda den varje gång VOS:et scheduleras.

# 9
## 9.1
Uppifrån och ner: code, read only data, global data för process gurka. heap för processen. minne som används för ett delat bibliotek. stack för processen.

## 9.2
Kommer skrivas ut att "child: x is at <pekare>" "mother: x is at <pekare>" där de pekar till samma address. Allting kopieras när child processen skapas men de virtuella adresserna sitter likadant. Ordningen av utskrifterna kan skilja sig eftersom wait sker efter mamman skrivit.
  
## 9.3
Man kan köra en mkfifo vilket öppnar en speciell typ av "first in first out" fil. Om man öppnar denna som writeonly så skriver man till den, och readonly så läser man från den. Den andra processen kan alltså öpnna denna som readonly och läsa från den, och det kommer fungera som en pipe.

## 9.4
Falskt. Vi använder opendir/readdir för mappar och open/read för en fil. En mapp har samma representation som en fil men är konceptuellt annorlunda (e.g har en speciell struktur, skrivning och läsning till en mapp betyder något annat än detsamma för en fil).

## 9.5
brk och dra tillbaka pekaren som motsvarar toppen av heapen med så mycket som man vill dra tillbaka med.

## 9.6
Vi hade behövt sätta denna entry längst bak i en länkad lista som är sorterad efter hur nyligen varje entry har accessats. Men för att göra det hade vi behövt leta rätt på elementet i den länkade listan, vilket tar tid.

## 9.7 *
execlp kommer bara köras en gång. Hem-mappen kommer aldrig skrivas ut eftersom processen inte längre kommer återgå till denna kod -- den har helt morphats till "ls"-programmet. Den komemr bara skriva ut current directory och sedan terminera.

## 9.8 *
Har skippat

## 9.9 *
Dom kan dela minne exempelvis om de båda använder ett delat bibliotek eller om bägge kör samma kod. Men då kan de inte skriva till dessa utan bara läsa. Vet dock inte hur man kan få dem att dela minne som bägge kan skriva till.

## 9.10 *
Fördelen med det är att det är mycket snabbare att göra context switch manuellt på user level (utan en massa systemanrop och generella heuristiker som inte är anpassade just för oss). En nackdel är att vi inte kan ha flera trådar som _kernel_ hanterar (dvs allt detta måste köras på en tråd), så vi kan inte utnyttja flera cores om vi hade det.
