# 1
## 1.1
The error is that we return an array that was allocated on the stack. The array was popped from the stack when we returned, so this memory may be overwritten in the future. Accessing this array can thus lead to garbage data being accessed, since it might have gotten overwritten by further usasge of the stack.

## 1.2
The first row is the code section, where the executable code resides. The second row is the second where constants are placed (constants may also be placed in the third row). The third row is the global section, where global variables are stored. The fourth row is the heap, where data that is dynamically allocated is stored. The fifth row is for shared data (libraries and such). The sixth row is the stack, which stores function variables.

## 1.3

## 1.4 
We can perform coalescing. We can simply iterate through the elements of the linked list. If two neighboring elements are free, we can coalesce these, forming a larger block. This can be done by simply changing the size of the first element in the neighboring pair. Having an ordered linked list makes the coalescing time complexity linear in the number of free blocks, which is better than if the blocks where unordered.

## 1.5 *

## 1.6 *

# 2
## 2.1
If we create two threads that both execute that procedure concurrently, then we will write "the count is 10" twice on two lines. The global variable "loop" could be a cause for concern, but the global variable (the only variable shared between the threads) is never modified, so we are good. Also, each thread has its own separate stack, and each function call generates its own stack frame, so no stack variables are shared. But the motivation about global variables is sufficient.

## 2.2

## 2.3 *

# 3
## 3.1
The states are "ready", "running", "blocked" and "finished". A process moves from ready to running by being scheduled. A process moves from running to ready if the scheduler decides to do so during a context switch. A process moves from running to blocked if some IO operation is blocking progression, and is moved to ready when that IO operation is resolved. A process is moved from running to finished when it is... finished.

## 3.2
The parameter we choose is how often timer interrupts should occur (measured in time). If the timer interrupt occurs too often (i.e the parameter is too low), the overhead of context switching and interrupt handling may become too costly. If the parameter is too high, then the reaction time will become worse.

## 3.3 *

# 4
## 4.1
Paging avoids external fragmentation by being able to quickly coalesce pages into larger blocks when a block is released, for instance by using a buddy system (you can then change a certain bit to identify a corresponding buddy at a certain level). The risk is that we may introduce more internal fragmentation, since the block sizes that can be allocated are only powers of two.

## 4.2
We can approximate LRU by implementing a clock algorithm. We could store a bit for each page that signifies whether it has been recently accessed. At first, it is 0. The algorithm would then "in a circular motion" iterate through the pages in main memory. If the page is chosen by the algorithm, the bit is flipped to 1. This can only be reset to 0 if the page is used again. If the page is not used and the algorithm chooses it again, it is evicted from main memory to make space for another page. The "bit" can be stored directly in hardware.

## 4.3 *

# 5 
## 5.1 
In a directory entry, we can find the following things: The inumber of the file, the record length of the file, the length of the string name and the corresponding high-level name. The inumber, name and record length are especially important. We cannot find information such as the size of the actual file; we would need to access the inode to find that. 

## 5.2
A file is "removed" when all hard links to it are removed. The actual contents of the file is not wiped. Instead, we simply free the data blocks and inode block by modifying the corresponding bitmaps.

## 5.3 *
